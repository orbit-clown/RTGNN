{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GATConv,GCNConv\n",
    "from torch.nn.functional import l1_loss\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:2')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device:', device)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameters\n",
    "win_size = 60 # 15,30,45,60\n",
    "temporal_win = 5 # 1,3,5,15\n",
    "csi_last_date=['2022-12-09','2022-11-18','2022-10-28','2022-09-30']\n",
    "nas_last_date=['2022-12-08','2022-11-16','2022-10-26','2022-10-05']\n",
    "dataset_name = 'hs300'\n",
    "\n",
    "if dataset_name == 'hs300':\n",
    "    input_dim = 6 #6 for hs300,5 for nas100\n",
    "    stock_num =  283 #283 for hs300,98 for nas100\n",
    "    select_num = 30\n",
    "    feature_cols=['close','open','high','low','turnover','volume']\n",
    "    last_date= csi_last_date[win_size//15-1]\n",
    "    m = 1.7  # 2\n",
    "else :\n",
    "    input_dim = 5 #6 for hs300,5 for nas100\n",
    "    stock_num =  98 #283 for hs300,98 for nas100\n",
    "    select_num = 10\n",
    "    feature_cols=['close','high','low','open','volume']\n",
    "    last_date= nas_last_date[win_size//15-1]\n",
    "    m = 0.8\n",
    "horizon = 1\n",
    "hidden_dim = 32\n",
    "out_dim = 1 \n",
    "heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading...\n"
     ]
    }
   ],
   "source": [
    "print(\"data loading...\")\n",
    "df = pd.read_csv(f'./dataset/{dataset_name}_{horizon}.csv')\n",
    "\n",
    "def filter_extreme_3sigma(series, n=3):\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    max_range = mean + n * std\n",
    "    min_range = mean - n * std\n",
    "    return np.clip(series, min_range, max_range)\n",
    "\n",
    "def standardize_zscore(series):\n",
    "    std = series.std()\n",
    "    mean = series.mean()\n",
    "    return (series - mean) / std\n",
    "    \n",
    "def process_features(df_features, feature_cols):\n",
    "    df_features_grouped = df_features.groupby('dt')\n",
    "    res = []\n",
    "    for dt in df_features_grouped.groups:\n",
    "        df = df_features_grouped.get_group(dt)\n",
    "        processed_df = process_daily_df_std(df, feature_cols)\n",
    "        res.append(processed_df)\n",
    "    df_features = pd.concat(res)\n",
    "    df_features = df_features.dropna(subset=feature_cols)\n",
    "    return df_features\n",
    "\n",
    "\n",
    "def process_daily_df_std(df, feature_cols):\n",
    "    df = df.copy()\n",
    "    for c in feature_cols:\n",
    "        df[c] = df[c].replace([np.inf, -np.inf], 0)\n",
    "        df[c] = filter_extreme_3sigma(df[c])\n",
    "        df[c] = standardize_zscore(df[c])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing...\n"
     ]
    }
   ],
   "source": [
    "print('data processing...')\n",
    "standardize_df = process_features(df,feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, win_size, horizon, mode):\n",
    "        self.win_size = win_size\n",
    "        self.horizon = horizon\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.dataset =  standardize_df\n",
    "        self.dataset['dt'] = pd.to_datetime(self.dataset['dt']) \n",
    "        self.stocks = self.dataset['kdcode'].unique()\n",
    "\n",
    "        # Preprocess data\n",
    "        self.feature_slices, self.label_slices = self.preprocess_data()\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        feature_slices = []\n",
    "        label_slices = []\n",
    "        \n",
    "        for stock in self.stocks:\n",
    "            stock_data = self.dataset[self.dataset['kdcode'] == stock] \n",
    "\n",
    "            train_data = stock_data[stock_data['dt'] < pd.to_datetime('2022-10-31')]\n",
    "            val_data = stock_data[(stock_data['dt'] < pd.to_datetime('2023-01-01')) & (stock_data['dt'] > pd.to_datetime('2022-09-21'))]\n",
    "            test_data = stock_data[stock_data['dt']> pd.to_datetime(last_date)] #2022-11-18 for hs300\n",
    "            if self.mode == 'train':\n",
    "                data = train_data\n",
    "            elif self.mode == 'test':\n",
    "                data = test_data\n",
    "            elif self.mode == 'val':\n",
    "                data = test_data\n",
    "            \n",
    "            feature_data=data.loc[:, feature_cols]\n",
    "            label_data = np.expand_dims(data['OT'].values, axis=1)\n",
    "            feature_slices.append([feature_data[i:i+self.win_size] for i in range(len(data)-self.win_size)])\n",
    "            label_slices.append([label_data[i+self.win_size] for i in range(len(data)-self.win_size)])\n",
    "        self.date=data['dt']\n",
    "        \n",
    "        feature_slices = np.transpose(np.array(feature_slices),(1,0,2,3))\n",
    "        label_slices = np.transpose(np.array(label_slices),(1,0,2))\n",
    "        return feature_slices, label_slices\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.FloatTensor(self.feature_slices[index]), torch.FloatTensor(self.label_slices[index])  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_slices)\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    feature_slices = [item[0] for item in batch]\n",
    "    label_slices = [item[1] for item in batch]\n",
    "\n",
    "    feature_slices=torch.stack(feature_slices, dim=0).permute(1,0,2,3)\n",
    "    label_slices=torch.stack(label_slices, dim=0).squeeze(-1).permute(1,0)\n",
    "    return feature_slices.view(feature_slices.shape[0],-1,feature_slices.shape[3]),label_slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= StockDataset(win_size,horizon ,'train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset= StockDataset(win_size,horizon ,'val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = StockDataset(win_size,horizon,'test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairNorm(nn.Module):\n",
    "    def __init__(self, mode='PN', scale=1):\n",
    "        assert mode in ['None', 'PN', 'PN-SI', 'PN-SCS']\n",
    "        super(PairNorm, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == 'None':\n",
    "            return x\n",
    "        col_mean = x.mean(dim=0)\n",
    "        if self.mode == 'PN':\n",
    "            x = x - col_mean\n",
    "            rownorm_mean = (1e-6 + x.pow(2).sum(dim=1).mean()).sqrt()\n",
    "            x = self.scale * x / rownorm_mean\n",
    "        if self.mode == 'PN-SI':\n",
    "            x = x - col_mean\n",
    "            rownorm_individual = (1e-6 + x.pow(2).sum(dim=1, keepdim=True)).sqrt()\n",
    "            x = self.scale * x / rownorm_individual\n",
    "        if self.mode == 'PN-SCS':\n",
    "            rownorm_individual = (1e-6 + x.pow(2).sum(dim=1, keepdim=True)).sqrt()\n",
    "            x = self.scale * x / rownorm_individual - col_mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim,hidden_dim,heads,t_att_heads):\n",
    "        super(Model, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(win_size,t_att_heads)\n",
    "        self.gru1 = nn.GRU(input_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.gru2 = nn.GRU(input_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.gru3 = nn.GRU(input_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.gat1 = GATConv(hidden_dim*win_size, hidden_dim, heads=heads) \n",
    "        self.gat2 = GATConv(hidden_dim*win_size, hidden_dim, heads=heads) \n",
    "        self.pn = PairNorm(mode='PN-SI')\n",
    "        self.predictor = nn.Sequential(nn.Linear(in_features=hidden_dim*heads, out_features=out_dim,bias=True),\n",
    "                                       nn.Sigmoid())\n",
    "                                       #nn.Tanh())\n",
    "                                       #nn.Softsign())\n",
    "        \n",
    "        self.alpha = torch.nn.Parameter(torch.FloatTensor([0.5]), requires_grad=True)\n",
    "        self.decay = torch.nn.Parameter(torch.FloatTensor([0.5]), requires_grad=True)\n",
    "        \n",
    "    def generate_edge_index(self, adj):\n",
    "        I = torch.eye(adj.size(0)).to(device)\n",
    "        adj+=I\n",
    "        # 得到所有大于阈值的元素坐标\n",
    "        indices = torch.nonzero(adj > adj.mean()+m*adj.std())\n",
    "\n",
    "        # 转换为边索引\n",
    "        edge_index = indices.t()\n",
    "\n",
    "        return edge_index\n",
    "\n",
    "    def forward(self, data, hs1):\n",
    "        data = data.permute(0, 2, 1)\n",
    "        hs1 = hs1.permute(0, 2, 1)\n",
    "        embedding1, _ = self.multihead_attn(data, data, data)\n",
    "        embedding1 = embedding1.permute(0, 2, 1)\n",
    "\n",
    "        embedding2, _ = self.multihead_attn(hs1, hs1, hs1)\n",
    "        embedding2 = embedding2.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "        out_GRU_1, _ = self.gru1(embedding1)\n",
    "        out_GRU_2, _ = self.gru2(embedding1)\n",
    "        out_GRU_3, _ = self.gru3(embedding2)\n",
    "        \n",
    "        M1 = torch.tanh(out_GRU_1.reshape(out_GRU_1.size(0), -1))\n",
    "        M2 = torch.tanh(out_GRU_2.reshape(out_GRU_2.size(0), -1))\n",
    "        M3 = torch.tanh(out_GRU_3.reshape(out_GRU_3.size(0), -1))\n",
    "        \n",
    "        adjacency_matrix_1 = F.relu(torch.tanh(torch.matmul(M1, M2.t()) - torch.matmul(M2, M1.t())))\n",
    "        adjacency_matrix_2 = F.relu(torch.tanh(torch.matmul(M1, M3.t()) - torch.matmul(M3, M1.t())))\n",
    "\n",
    "        edge_index_1 = self.generate_edge_index(adjacency_matrix_1)\n",
    "        edge_index_2 = self.generate_edge_index(adjacency_matrix_2)\n",
    "\n",
    "        GAT_output_1 = self.gat1((M1+M2)/2, edge_index_1)\n",
    "        GAT_output_2 = self.gat2((M1+M3)/2, edge_index_2)\n",
    "\n",
    "        embedding = self.pn(GAT_output_1*self.alpha+GAT_output_2*self.decay)\n",
    "        output = self.predictor(embedding)\n",
    "\n",
    "        return output,adjacency_matrix_1,edge_index_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(preds, labels, margin=0.1):\n",
    "    preds = preds.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    \n",
    "    assert preds.size() == labels.size()\n",
    "\n",
    "    # 创建一个矩阵，其中每个元素i,j都是预测值或标签差值\n",
    "    diff_preds = preds[None,:] - preds[:, None]\n",
    "    diff_labels = labels[None, :] - labels[:, None]\n",
    "    \n",
    "    # 使用几何余弦表示一致性，当预测的差值和真实标签的差值同号时，为1，否则为0\n",
    "    mask = (diff_preds * diff_labels < 0).type(torch.float32)\n",
    "    \n",
    "    # 对差值预测应用一个线性关系，并引入一个margin\n",
    "    hinge_loss = torch.nn.functional.relu(margin - diff_preds).pow(2)\n",
    "    \n",
    "    # 乘以mask以获得最后的损失，只有不一致的对才会有损失值\n",
    "    loss = mask * hinge_loss\n",
    "    return loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_loss = nn.MSELoss()\n",
    "alpha = 1\n",
    "beta= 2e-5\n",
    "def combined_loss(preds,labels):\n",
    "    return alpha*MSE_loss(preds,labels)+beta*pairwise_ranking_loss(preds,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80, train loss: 0.19772631370297836\n",
      "Epoch 2/80, train loss: 0.08515562067953451\n",
      "Epoch 3/80, train loss: 0.05245809786827709\n",
      "Epoch 4/80, train loss: 0.09569480809282345\n",
      "Epoch 5/80, train loss: 0.06300803429218584\n",
      "Epoch 6/80, train loss: 0.06257990393836768\n",
      "Epoch 7/80, train loss: 0.0498722273915377\n",
      "Epoch 8/80, train loss: 0.04770237303739487\n",
      "Epoch 9/80, train loss: 0.04090680901973574\n",
      "Epoch 10/80, train loss: 0.0337907151026666\n",
      "Epoch 11/80, train loss: 0.07311191306619028\n",
      "Epoch 12/80, train loss: 0.08943955907917589\n",
      "Epoch 13/80, train loss: 0.10157731217688495\n",
      "Epoch 14/80, train loss: 0.08728860807175058\n",
      "Epoch 15/80, train loss: 0.06749592812382924\n",
      "Epoch 16/80, train loss: 0.07053738640803146\n",
      "Epoch 17/80, train loss: 0.040108238464825856\n",
      "Epoch 18/80, train loss: 0.031529136252709936\n",
      "Epoch 19/80, train loss: 0.027903163918640495\n",
      "Epoch 20/80, train loss: 0.04844617114788152\n",
      "Epoch 21/80, train loss: 0.04679032437758899\n",
      "Epoch 22/80, train loss: 0.04029803343195871\n",
      "Epoch 23/80, train loss: 0.039132289327975946\n",
      "Epoch 24/80, train loss: 0.02786549832426149\n",
      "Epoch 25/80, train loss: 0.020143829114086395\n",
      "Epoch 26/80, train loss: 0.019183222032434707\n",
      "Epoch 27/80, train loss: 0.01978719736250378\n",
      "Epoch 28/80, train loss: 0.01691821383651454\n",
      "Epoch 29/80, train loss: 0.018411401126012795\n",
      "Epoch 30/80, train loss: 0.017948222722573143\n",
      "Epoch 31/80, train loss: 0.01689804109958514\n",
      "Epoch 32/80, train loss: 0.014741881824243509\n",
      "Epoch 33/80, train loss: 0.020162260830697252\n",
      "Epoch 34/80, train loss: 0.016020857203152217\n",
      "Epoch 35/80, train loss: 0.01469824379334585\n",
      "Epoch 36/80, train loss: 0.014129372972179213\n",
      "Epoch 37/80, train loss: 0.013318709134662371\n",
      "Epoch 38/80, train loss: 0.013578321359619144\n",
      "Epoch 39/80, train loss: 0.013565010344486752\n",
      "Epoch 40/80, train loss: 0.015902234781975798\n",
      "Epoch 41/80, train loss: 0.01823068807997537\n",
      "Val Loss: 0.0169566457403275,Rank IC:-0.0036218313525726215,Rank ICIR:-0.043699827092374445\n",
      "Epoch 42/80, train loss: 0.014878683202558425\n",
      "Val Loss: 0.015818662721184526,Rank IC:-0.020428237595502854,Rank ICIR:-0.18635930017051974\n",
      "Epoch 43/80, train loss: 0.013195927965019812\n",
      "Val Loss: 0.012730980272640578,Rank IC:0.008395645954017825,Rank ICIR:0.10402336838083637\n",
      "Epoch 44/80, train loss: 0.011248693386176331\n",
      "Val Loss: 0.010862175282477344,Rank IC:0.014910686686969642,Rank ICIR:0.14429763582220165\n",
      "Epoch 45/80, train loss: 0.011589053913823139\n",
      "Val Loss: 0.011857213390627837,Rank IC:0.006479860040529946,Rank ICIR:0.09044439725431716\n",
      "Epoch 46/80, train loss: 0.012387872619510168\n",
      "Val Loss: 0.010349859298048426,Rank IC:-0.01915740069199705,Rank ICIR:-0.13261898251633217\n",
      "Epoch 47/80, train loss: 0.010835898811572616\n",
      "Val Loss: 0.01145589544101622,Rank IC:0.010600441451753009,Rank ICIR:0.12636578149895183\n",
      "Epoch 48/80, train loss: 0.011954860115487688\n",
      "Val Loss: 0.01067357011046647,Rank IC:-0.017307289164208872,Rank ICIR:-0.12030441272485386\n",
      "Epoch 49/80, train loss: 0.01138319106561213\n",
      "Val Loss: 0.010689516080334721,Rank IC:0.0009606730231138462,Rank ICIR:0.009293244016035852\n",
      "Epoch 50/80, train loss: 0.01048593186112342\n",
      "Val Loss: 0.010162859395749218,Rank IC:-0.007333100433953026,Rank ICIR:-0.08099734706759389\n",
      "Epoch 51/80, train loss: 0.010169118274209682\n",
      "Val Loss: 0.010421343553517617,Rank IC:0.003228525790369374,Rank ICIR:0.03418417665160504\n",
      "Epoch 52/80, train loss: 0.010369241108631317\n",
      "Val Loss: 0.009727657592905632,Rank IC:-0.016508641598584034,Rank ICIR:-0.1648871545406011\n",
      "Epoch 53/80, train loss: 0.01011418864514315\n",
      "Val Loss: 0.010060057921250084,Rank IC:-0.003293691938044668,Rank ICIR:-0.03042899881992469\n",
      "Epoch 54/80, train loss: 0.010322583728718098\n",
      "Val Loss: 0.009858938811261871,Rank IC:-0.018029176036510535,Rank ICIR:-0.15231642635791112\n",
      "Epoch 55/80, train loss: 0.010054965247953001\n",
      "Val Loss: 0.009818246093466569,Rank IC:0.0014884286501339033,Rank ICIR:0.0161543524370835\n",
      "Epoch 56/80, train loss: 0.01039000385315563\n",
      "Val Loss: 0.009951044442093472,Rank IC:-0.005774939749810342,Rank ICIR:-0.050307458569881496\n",
      "Epoch 57/80, train loss: 0.00982014315527198\n",
      "Val Loss: 0.01023315967744626,Rank IC:-0.00015315749350621542,Rank ICIR:-0.001565479652475669\n",
      "Epoch 58/80, train loss: 0.009721546527244013\n",
      "Val Loss: 0.010268055101570623,Rank IC:0.012298341817392474,Rank ICIR:0.12930439868262059\n",
      "Epoch 59/80, train loss: 0.00959175189165767\n",
      "Val Loss: 0.009540681114721719,Rank IC:-0.0035922800426459846,Rank ICIR:-0.040030351518422366\n",
      "Epoch 60/80, train loss: 0.009532835065463366\n",
      "Val Loss: 0.009229738868123763,Rank IC:-0.017681298094623817,Rank ICIR:-0.16541243849580572\n",
      "Epoch 61/80, train loss: 0.009460757672049519\n",
      "Val Loss: 0.009340498957752068,Rank IC:0.00826354843471643,Rank ICIR:0.08444597465519484\n",
      "Epoch 62/80, train loss: 0.00946848497708822\n",
      "Val Loss: 0.009252191567600269,Rank IC:0.01736999695508973,Rank ICIR:0.18474923577498512\n",
      "Epoch 63/80, train loss: 0.00936281418373764\n",
      "Val Loss: 0.009104884368719155,Rank IC:0.0051165525068374304,Rank ICIR:0.055742462326939406\n",
      "Epoch 64/80, train loss: 0.009315165762339858\n",
      "Val Loss: 0.009049408124272879,Rank IC:-0.003087229090822372,Rank ICIR:-0.028717520919894757\n",
      "Epoch 65/80, train loss: 0.009288840540156437\n",
      "Val Loss: 0.008974734388477328,Rank IC:-0.0210406037924252,Rank ICIR:-0.18802378690909397\n",
      "Epoch 66/80, train loss: 0.00922662520653379\n",
      "Val Loss: 0.008898918760391686,Rank IC:-0.018514517696921532,Rank ICIR:-0.17879702691096658\n",
      "Epoch 67/80, train loss: 0.009144497558592491\n",
      "Val Loss: 0.008824794390915092,Rank IC:-0.008248690283514265,Rank ICIR:-0.07638564274858409\n",
      "Epoch 68/80, train loss: 0.009103719605806242\n",
      "Val Loss: 0.008692517401302752,Rank IC:0.009195480327145258,Rank ICIR:0.09944160500647047\n",
      "Epoch 69/80, train loss: 0.009091185685984377\n",
      "Val Loss: 0.00874933173794964,Rank IC:-0.01335078473808337,Rank ICIR:-0.1277547741004367\n",
      "Epoch 70/80, train loss: 0.009081701413878193\n",
      "Val Loss: 0.008730734147796121,Rank IC:0.0015199574076258406,Rank ICIR:0.014946607592222938\n",
      "Epoch 71/80, train loss: 0.00903777945860434\n",
      "Val Loss: 0.00875275016325638,Rank IC:0.00908668531949713,Rank ICIR:0.10195614614964509\n",
      "Epoch 72/80, train loss: 0.008993723904001367\n",
      "Val Loss: 0.008622491832714357,Rank IC:0.017172329198077846,Rank ICIR:0.18403869640665715\n",
      "Epoch 73/80, train loss: 0.00897281602305166\n",
      "Val Loss: 0.008583820525448599,Rank IC:0.009272463242657724,Rank ICIR:0.08988737805367414\n",
      "Epoch 74/80, train loss: 0.008957634170080159\n",
      "Val Loss: 0.008513657401288817,Rank IC:0.021073496511737087,Rank ICIR:0.17863206097639187\n",
      "Epoch 75/80, train loss: 0.008972379922041163\n",
      "Val Loss: 0.008491192026550715,Rank IC:0.03458857904975053,Rank ICIR:0.277170352010608\n",
      "Epoch 76/80, train loss: 0.008954272924929778\n",
      "Val Loss: 0.008564227227801728,Rank IC:0.011048212832867693,Rank ICIR:0.09221228653036485\n",
      "Epoch 77/80, train loss: 0.008914029639764716\n",
      "Val Loss: 0.008450564748657814,Rank IC:0.0263699992072847,Rank ICIR:0.2287316781994734\n",
      "Epoch 78/80, train loss: 0.008963075420993692\n",
      "Val Loss: 0.008473498097532024,Rank IC:0.02364173984879031,Rank ICIR:0.21663676321754993\n",
      "Epoch 79/80, train loss: 0.008896046988960935\n",
      "Val Loss: 0.00851443649899539,Rank IC:0.016305053445497507,Rank ICIR:0.14920254023112012\n",
      "Epoch 80/80, train loss: 0.008922647684812546\n",
      "Val Loss: 0.00836693284128835,Rank IC:0.044044767792277194,Rank ICIR:0.2873825763814815\n",
      "4121.260460526316\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "\n",
    "\n",
    "epochs = 80\n",
    "model = Model(input_dim=input_dim,hidden_dim=hidden_dim,heads=heads, t_att_heads=win_size//temporal_win).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "ess= []\n",
    "max_ic = 0\n",
    "for epoch in range(epochs):   \n",
    "    model.train()\n",
    "    loss_return = 0 \n",
    "    es= []\n",
    "    #adjacency_matrices = []\n",
    "    for i, (feature_batch, label_batch) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        feature = feature_batch.to(device)\n",
    "        if i == 0:\n",
    "            hs1 = feature\n",
    "        target = label_batch.to(device)\n",
    "        output, a , e = model(feature,hs1)\n",
    "        es.append(e.shape[1])\n",
    "        hs1 = feature\n",
    "        loss = combined_loss(output, target)\n",
    "        loss.backward()\n",
    "        loss_return+=loss.item()\n",
    "        optimizer.step()\n",
    "        # if epoch % 15 == 0:\n",
    "        #     adjacency_matrices.append(a.detach().cpu())\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, train loss: {loss_return/i}')\n",
    "    ess.append(np.mean(es))\n",
    "    if (epoch+1) > 40:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_total_loss = 0\n",
    "            preds = []\n",
    "            targets = []\n",
    "            for j, (val_data, val_target) in enumerate(val_loader):\n",
    "\n",
    "                val_data = val_data.to(device)\n",
    "                val_target = val_target.to(device)\n",
    "\n",
    "                if j==0:\n",
    "                    history = val_data\n",
    "\n",
    "                val_output,_,_ = model(val_data,history)\n",
    "                history = val_data\n",
    "                val_loss = combined_loss(val_output, val_target)\n",
    "                val_total_loss += val_loss.item()\n",
    "                preds.append(np.array(val_output.squeeze().tolist()))\n",
    "                targets.append(np.array(val_target.squeeze().tolist()))\n",
    "                del val_output, val_target\n",
    "        preds = np.array(preds)\n",
    "        targets = np.array(targets)\n",
    "        daily_rankICs = np.array([spearmanr(rankdata(preds[i]), rankdata(targets[i])).correlation for i in range(preds.shape[0])])\n",
    "        mean_rankIC = np.mean(daily_rankICs)\n",
    "        std_dev_rankIC = np.std(daily_rankICs)\n",
    "        RankICIR = mean_rankIC / std_dev_rankIC\n",
    "        if mean_rankIC > max_ic and val_total_loss/j< 0.01:\n",
    "            max_ic = mean_rankIC\n",
    "            torch.save(model, f=f\"models/HTGNN_{dataset_name}_{horizon}_best.pth\")\n",
    "        print(f'Val Loss: {val_total_loss/j},Rank IC:{mean_rankIC},Rank ICIR:{RankICIR}') \n",
    "    # if epoch % 15 == 0:\n",
    "    #     total_adjacency_matrix = np.sum(adjacency_matrices, axis=0)\n",
    "    #     min_val = np.min(total_adjacency_matrix)\n",
    "    #     max_val = np.max(total_adjacency_matrix)\n",
    "    #     np.fill_diagonal(total_adjacency_matrix, 0)\n",
    "    #     minmax_normalized_adjacency_matrix = (total_adjacency_matrix - min_val) / (max_val - min_val)\n",
    "    #     plt.figure(figsize=(10,10))\n",
    "    #     sns.heatmap(minmax_normalized_adjacency_matrix, cmap='Blues')\n",
    "    #     plt.savefig(f'train_{dataset_name}_{epoch+1}.png',dpi=600)\n",
    "    #     plt.show()\n",
    "print(np.mean(ess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(f\"models/HTGNN_hs300_best.pth\"), strict=False)\n",
    "model = torch.load(f\"models/HTGNN_{dataset_name}_{horizon}_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model testing...\n",
      "Test MSE: 0.000578341720368775\n",
      "Test MAE: 0.018750914387961393\n"
     ]
    }
   ],
   "source": [
    "# After training, test the model\n",
    "print(\"model testing...\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_total_loss = 0\n",
    "    total_mae = 0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for i, (test_data, test_target) in enumerate(test_loader):\n",
    "\n",
    "        test_data = test_data.to(device)\n",
    "        test_target = test_target.to(device)\n",
    "\n",
    "        if i==0:\n",
    "            history = test_data\n",
    "\n",
    "        test_output,_,_ = model(test_data,history)\n",
    "        history = test_data\n",
    "        test_loss = MSE_loss(test_output, test_target)\n",
    "\n",
    "        test_total_loss += test_loss.item()\n",
    "        total_mae += l1_loss(test_output, test_target).item()\n",
    "        preds.append(np.array(test_output.squeeze().tolist()))\n",
    "        targets.append(np.array(test_target.squeeze().tolist()))\n",
    "        del test_output, test_target\n",
    "    # Convert lists to numpy arrays for scipy\n",
    "    preds = np.array(preds)\n",
    "    targets = np.array(targets)\n",
    "print(f'Test MSE: {test_total_loss/i}') \n",
    "print(f'Test MAE: {total_mae/i}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.044044767792277194 0.2873825763814815\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, rankdata\n",
    "\n",
    "# 计算每一天的 RankIC\n",
    "daily_rankICs = np.array([spearmanr(rankdata(preds[i]), rankdata(targets[i])).correlation for i in range(preds.shape[0])])\n",
    "\n",
    "# 计算 RankICIR\n",
    "mean_rankIC = np.mean(daily_rankICs)\n",
    "std_dev_rankIC = np.std(daily_rankICs)\n",
    "RankICIR = mean_rankIC / std_dev_rankIC\n",
    "\n",
    "print(mean_rankIC,RankICIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = np.empty((0, select_num))  # 初始化一个空的二维数组\n",
    "\n",
    "for i in range(targets.shape[0]):\n",
    "    select = np.argsort(preds[i])[::-1][:select_num]\n",
    "    hold = np.vstack((hold, test_dataset.stocks[select]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dateset.date数据准备\n",
    "dates = test_dataset.date.iloc[win_size:]\n",
    "\n",
    "# 假设hold是一个numpy数组，将其转换为DataFrame\n",
    "hold_df = pd.DataFrame(hold)\n",
    "\n",
    "# 创建一个新DataFrame，包含日期和hold数据\n",
    "combined_df = pd.concat([dates.reset_index(drop=True), hold_df], axis=1)\n",
    "\n",
    "# 重命名列名\n",
    "cols = ['dt'] + [f'kdcode{i}' for i in range(1, select_num+1)]\n",
    "combined_df.columns = cols\n",
    "\n",
    "# 保存为CSV文件\n",
    "combined_df.to_csv(f'./data/hold_HTGNNpro_{dataset_name}_{horizon}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "stock_df = pd.read_csv(f'./dataset/{dataset_name}_1.csv')\n",
    "hold_df = pd.read_csv(f'./data/hold_HTGNNpro_{dataset_name}_{horizon}.csv')\n",
    "\n",
    "output = []\n",
    "# 迭代hold数据\n",
    "for index, row in hold_df.iterrows():\n",
    "    kdcode_columns = [f'kdcode{i}' for i in range(1, select_num+1)]\n",
    "    kd_codes = row[kdcode_columns].values\n",
    "    dt = row['dt']\n",
    "\n",
    "    # 获取对应的 OT 值\n",
    "    ot_values = stock_df[(stock_df['kdcode'].isin(kd_codes)) & (stock_df['dt'] == dt)]['OT']\n",
    "\n",
    "    # 计算平均值\n",
    "    daily_return = ot_values.mean()\n",
    "\n",
    "    # 将日期和对应的收益存储起来\n",
    "    output.append([dt, daily_return])\n",
    "\n",
    "# 创建DataFrame并写入文件\n",
    "output_df = pd.DataFrame(output, columns=['datetime', 'daily_return'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv(f'./data/return_HTGNNpro_{dataset_name}_{horizon}.csv', index=False)\n",
    "#torch.save(model, f=f\"models/HTGNN_{dataset_name}_{horizon}_{mean_rankIC}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
